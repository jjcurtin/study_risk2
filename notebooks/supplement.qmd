---
title: "Supplemental Material"
author: "Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
format:
  html:
    embed-resources: true
params:
  
  study: "combined"
  model: "dyn_dem_oud" 
  version: "v8"
editor_options: 
  chunk_output_type: console
---

This file contains the supplemental materials for *Dynamic lapse risk prediction in a national sample of individuals with opioid use disorder using personal sensing and machine learning*. It includes all supplemental figures and tables.   
```{r}
#| include: false
study <- params$study
model <- params$model
version <- params$version
```


```{r}
#| include: false
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(patchwork)
library(kableExtra)

devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true")

path_shared <- format_path(str_c("risk2/data_processed/shared"))
path_models <- format_path(str_c("risk2/models/", study))


pp_dem <- read_csv(here::here(path_models, 
                              "pp_fairness_xgboost_contrast_full.csv"),
                   show_col_types = FALSE)

preds <- read_rds(here::here(path_models, "preds_xgboost_kfold_6_x_5_v8_dyn_dem_oud.rds"))


shaps <- read_rds(here::here(path_models, str_c("shapsgrp_xgboost_kfold_6_x_5_",
                                           version, "_", model, ".rds")))
```


## Protocol Deviations

We published this study's protocol as a registered report (International Registered Report Identifier [IRRID]: DERR1-10.2196/29563) during the initial enrollment of pilot participants. The registered report can be found at [https://www.researchprotocols.org/2021/12/e29563/](https://www.researchprotocols.org/2021/12/e29563/). For transparency, we documented changes made to the registered report below: 

### Sensing Data Streams  
<!--return to this section after decision is made to remove OUD id vars-->
Our candidate models included features derived from a subset of intake self-report measures, daily surveys, and geolocation sensing data (see Measures section). Consequently, we excluded cellular communication data, daily video check-ins, app usage data, and certain self-report measures. These decisions were informed by (1) our group's personal sensing work with alcohol use disorder, (2) technological constraints, and (3) the desire to balance feature diversity for capturing lapse complexity and minimizing participant burden and computational cost. 

1. In our alcohol use disorder research, daily surveys and geolocation sensing have shown moderate to excellent predictive signal (aurocs .72-.91<!--confirm claire auroc-->) [@wyantMachineLearningModels2024; @wyantForecastingRiskAlcoholunderreview<!--Cite claire gps paper-->]. Cellular communication sensing, however, has fallen short of these thresholds<!--meta paper-->. Moreover, Apple places strict restrictions on app access to communications, meaning inclusion of these features would result in a model that could only be deployed within an Android operating system. 
    
2. We discontinued collecting daily video check-ins about 6 months into the 2.5 year data collection due to technical issues and App usage data (beyond the required study tasks) were generally sparse and inconsistent across participants. 

3. Self-report measures can substantially increase data collection burden and expand the feature space. Therefore, we opted to not include all measures. Monthly surveys were lengthy (20–30 minutes) and, based on our work with alcohol use disorder, added no incremental predictive value beyond the dynamic sensing data. However, individual differences in severity of use and stability of recovery at the time of intake have demonstrated some predictive signal. Key demographic variables known to influence OUD treatment access and clinical outcomes are also important to reduce model bias by preventing these effects being encoded into the model indirectly by proxy variables. See Measures section in manuscript for a comprehensive list of all retained self-report items.

### Resampling Method
Our initial protocol proposed repeated cross-validation for model selection and a single held-out test set for evaluation. Although we nearly reached our recruitment goal (N = 451/480), the number of participants with usable data was substantially lower (N = 299). To maximize data use, we used nested cross-validation (6 repeats of 5-fold cross-validation on the outer loop and 2 repeats of 5-fold cross-validation on the inner loop). This resampling method still removes optimization bias by separating model selection and evaluation. Specifically, nested cross-validation embeds an inner cross-validation loop for model selection within an outer cross-validation loop for model evaluation. This approach makes more efficient use of data in that each observation contributes to model training and model evaluation during the cross-validation process. Moreover, final estimates are averaged over 30 test sets leading to less variance in the estimate compared to a single test set. 



## Daily Survey

This survey asks about your recent drug use, medication assisted treatment, mood, and daily experiences. Several questions ask about using drugs "for non-medical reasons" which means using drugs to feel intoxicated or high. It is possible to use a drug you are prescribed for non-medical reasons, for example, by taking more than prescribed.

1. Have you used any opioids for non-medical reasons that you have not yet reported?

    - No 
    - Yes 

    Please select the time(s) that you have not yet reported when you used opioids for non-medical reasons: [Display if Q1 is Yes]

    [TIME PICKER, MULTI-SELECT]



Thinking about the past 24 hours…

2. Which of these drugs have you used for non-medical reasons? Select all that apply.

    - Alcohol
    - Cannabis (marijuana, pot, grass, hash, K2, spice, etc.)
    - Stimulants (cocaine, meth, speed, ecstasy, molly, Adderall, etc.)
    - Inhalants (nitrous, glue, petrol, paint thinner, etc.)
    - Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)
    - Hallucinogens (LSD, acid, mushrooms, PCP, special K, etc.)
    - None of the above


3. Did you take your daily medication for opioid use (e.g., Suboxone, Methadone, Buprenorphine, etc.). as prescribed?

    - Yes
    - No, I missed a dose
    - No, I am using a monthly medication (e.g., Vivitrol)
    - No, I am no longer prescribed medication for opioid use


Thinking about the past 24 hours…

4. Think about the worst pain you experienced. How painful was it?

    - No pain 
    - Mildly 
    - Moderately 
    - Considerably 
    - Extremely

5. Think about the greatest urge you had to use opioids. How strong was it?

    - No urge
    - Mildly
    - Moderately
    - Considerably
    - Extremely

6. Think about the riskiest situation you experienced (people, places, or things that interfere with your recovery). How risky was it?

    - No risky situation
    - Mildly
    - Moderately
    - Considerably
    - Extremely

7. Think of the biggest hassle or most stressful event you experienced. How stressful was it?

    - No hassle/stressful event
    - Mildly
    - Moderately 
    - Considerably
    - Extremely

8. Think of the most pleasant or positive event you experienced. How pleasant was it?

    - No pleasant/positive event
    - Mildly
    - Moderately
    - Considerably
    - Extremely


Thinking about the past 24 hours…

9. How did you sleep?

    - Very badly
    - Badly
    - Neutral
    - Well
    - Very well

10. How depressed have you felt?

    - Not at all
    - Mildly
    - Moderately
    - Considerably
    - Extremely

11. How angry have you felt?

    - Not at all
    - Mildly
    - Moderately 
    - Considerably
    - Extremely

12. How anxious have you felt?

    - Not at all 
    - Mildly 
    - Moderately 
    - Considerably 
    - Extremely

13. How relaxed have you felt?

    - Not at all 
    - Mildly 
    - Moderately 
    - Considerably 
    - Extremely

14. How happy have you felt?

    - Not at all 
    - Mildly 
    - Moderately 
    - Considerably 
    - Extremely


Now thinking forward to the next week…

15. How motivated are you to completely avoid using opioids for non-medical reasons?

    - Not at all 
    - Mildly 
    - Moderately 
    - Considerably 
    - Extremely

16. How confident are you in your ability to completely avoid using opioids for non-medical reasons?

    - Not at all 
    - Mildly 
    - Moderately 
    - Considerably 
    - Extremely


Thank you! You have now completed the daily update.

## Data Exclusions

A total of 336 participants were eligible, consented, and remained on study for at least one month. Of these we excluded 37 participants' data from our analyses for the following reasons:

1 participant was excluded due to geolocation data indicating they were not residing in the United States. 

- During eda on EMA utc_offset it was determined they had modal UTC offset of 60 (+5 hours). This puts them in either Islamabad, Ekaterinburg, Karachi, or Tashkent.

7 participants were excluded due to evidence of careless responding on daily surveys. They all reported over 200 lapses while on study and had the highest number of duplicate lapse reports (accounting for 1177/1285 duplicate lapses). These subids also account for 62/66 lapses that started after the survey start time, but before survey complete time. This could mean they were stopping the survey to use opioids or could be potentially careless or inaccurate reporting. They also account for 64/107 missing lapse dates and times after responding yes to "Have you used any opioids for non-medical reasons that you have not yet reported?". A more precise description by participant is listed below.

- subid 1312 reported 1099 lapses over 359 days. Also reported 418 duplicate lapses not included in 1099 count. 

- subid 1427 reported 854 lapses over 364 days. Also reported 388 duplicate lapses not included in 854 count. They had 13 lapses that started after the survey start time (but before survey completion).

- subid 1132 reported 769 lapses over 360 days. Also reported 129 duplicate lapses not included in 769 count. They also had 27 of the 107 lapses with missing `lapse_date_time` values for `lapse_report` == "Yes". They had 8 lapses that started after the survey start time (but before survey completion).

- subid 1216 reported 680 lapses over 401 days. Also reported 96 duplicate lapses not included in 680 count.

- subid 1172 reported 420 lapses over 367 days. Also reported 60 duplicates not included in 420 count. They also had 37 of the 107 lapses with missing `lapse_date_time` values for `lapse_report` == "Yes". 

- subid 1379 reported 383 lapses over 265 days. They had 14 lapses that started after the survey start time (but before survey completion).

- subid 1101 reported 235 lapses over 144 days. Also reported 62 duplicate lapses not included in 235 count.


11 participants were excluded due to unusually low adherence. 

- 1 participant completed only 3 daily survey prompts over 88 days. All remaining subids have at least > 20% EMA adherence.

- 2 participants had no gps data.

- 8 participants had fewer than 20 geolocation points per day on average.


13 participants were excluded due to insufficient context data for geolocation points. We required participants have at least two contextualized locations other than their home. 

- one participant had no context data for any locations

- 12 participants did not have at least 2 known locations outside their home. Many only had home address reported and we do not gather follow-up context information about the home.


4 participant was excluded due to no longer having a goal of abstinence. 

- subid 1435 reported 141 lapses over 330 days. They reported at least one lapse on 119 days (equivalent to roughly 4/11 months on study). 

- subid 1195 reported 275 lapses over 173 days.

- subid 1257 reported 258 lapses over 364 days. 

- subid 1330 reported 243 lapses over 355 days.



## Supplemental Figures

### Figure S1: Global Feature Importance for all Feature Categories

```{r}
#| code-fold: true

shap_levels_all <- shaps |> 
  group_by(variable_grp) |>
  summarize(mean_value = mean(abs(value)), .groups = "drop") |>
  arrange(mean_value) |> 
  pull(variable_grp)

n_obs <- max(shaps$id_obs)

shaps_day_max <- shaps |>
  group_by(id_obs) |>
  slice_max(value) |> 
  group_by(variable_grp) |> 
  summarise(n = n(),
            prop = n/n_obs) |> 
  ungroup() 


global_panel_all <- shaps |>
  group_by(variable_grp) |>
  summarize(mean_value = mean(abs(value)), .groups = "drop") |> 
  mutate(variable_grp = factor(variable_grp, levels = shap_levels_all)) |> 
  ggplot(mapping = aes(x = variable_grp, y = mean_value)) +
  geom_bar(fill = "#240e31", 
           stat = "identity", position = "dodge") +
  labs(y = "Mean(|Shapley Value|)",
       x = NULL,
       fill = NULL) +
  theme_classic() +
  theme(axis.text=element_text(size=9.5),
        legend.key.size = unit(0.25, "cm"),
        panel.border = element_rect(colour = "black", fill = NA, linewidth = 1),
        legend.position = "right") +
  coord_flip()


local_panel_all <- shaps_day_max |> 
  right_join(shaps |> ungroup() |> select(variable_grp) |> distinct(), 
             by = "variable_grp") |> 
  mutate(prop = if_else(is.na(prop), 0, prop)) |> 
  mutate(variable_grp = factor(variable_grp, levels = shap_levels_all)) |>
  ggplot(mapping = aes(x = variable_grp, y = prop)) +
  geom_bar(fill = "#046B52", 
           stat = "identity", position = "dodge") +
  labs(y = "Proportion of days as top feature",
       x = NULL,
       fill = NULL) +
  theme_classic() +
  theme(axis.text=element_text(size=9.5),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.key.size = unit(0.25, "cm"),
        panel.border = element_rect(colour = "black", fill = NA, linewidth = 1),
        legend.position = "right") +
  coord_flip()
```


```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 10

global_panel_all + local_panel_all
```


### Figure S2: Individual Shapley Plots for all Feature Categories

```{r}
#| label: fig-2
#| fig-cap: "Feature importance partial dependence plots."
#| message: false
#| fig-width: 10
#| fig-height: 14
#| code-fold: true

shap_levels <- shaps |> 
  group_by(variable_grp) |>
  summarize(mean_value = mean(abs(value)), .groups = "drop") |>
  arrange(desc(mean_value)) |> 
  pull(variable_grp)

shaps|>
  filter(variable_grp %in% shap_levels) |>
  mutate(variable_grp = factor(variable_grp, levels = shap_levels)) |>
  ggplot(aes(x = rfvalue, y = value)) +
  geom_point(alpha = .3) +
  geom_smooth(method = "gam", formula = y ~ s(x, k = 10, bs = "cs"), se = FALSE) +
  facet_wrap(~ variable_grp, scales = "free", ncol = 5) +
  labs(
    title = "Top 30 SHAP Variable Relationships",
    x = "z-score raw feature score",
    y = "Shapley value"
  ) +
  theme_classic() +
  theme(
    strip.text = element_text(size = 8),
    plot.title = element_text(size = 14, face = "bold")
  )
```


## Supplemental Tables

### Table S1: Simple k-fold Performance by Statistical Algorithm


### Table S2: Demographic Contrasts for all Models

```{r}
#| label: tbl-fairness
#| tbl-cap: "Median difference in auROC, 95% Bayesian credible interval (CI), and posterior probability that that the auROC difference was smaller or larger than 0 for fairness contrasts."
#| code-fold: true

footnote_table_fair <- "Median auROC differences less than 0 indicate the model, on average, performed worse for the disadvantaged group (not male, non-White and/or Hispanic, income below poverty line, small town/rural, high school or less) compared to the advantaged group (male, non-Hispanic White, income above poverty line, urban/suburban, some college). Bayesian CI represents the range of values where there is a 95% probability that the true auROC difference lies within that range. Probability indicates the posterior probability that this difference is smaller or larger than 0 (i.e., the models are performing differently for fairness subgroups)."


pp_sex <- pp_dem |> 
  filter(contrast == "not male vs male") |> 
   mutate(ci = str_c("[", round(lower, 3), ", ", round(upper, 3), "]"),
         median = as.character(round(median, 3)),
         probability = as.character(round(probability, 3))) |> 
  select(contrast, median, ci, probability) |> 
  rename(Median = median,
         `Bayesian CI` = ci,
         Probability = probability)

pp_income <- pp_dem |> 
  filter(contrast == "below poverty vs above poverty") |> 
   mutate(ci = str_c("[", round(lower, 3), ", ", round(upper, 3), "]"),
         median = as.character(round(median, 3)),
         probability = as.character(1 - round(probability, 3))) |> 
  select(contrast,median, ci, probability) |> 
  rename(`Median` = median,
         `Bayesian CI` = ci,
         `Probability` = probability)

pp_race <- pp_dem |> 
  filter(contrast == "Hispanic and/or not white vs non-Hispanic White") |> 
  mutate(contrast = "Hispanic and/or not White vs non-Hispanic White") |> 
   mutate(ci = str_c("[", round(lower, 3), ", ", round(upper, 3), "]"),
         median = as.character(round(median, 3)),
         probability = as.character(sprintf("%.3f", probability))) |> 
  select(contrast, median, ci, probability) |> 
  rename(`Median` = median,
         `Bayesian CI` = ci,
         `Probability` = probability)

pp_race <- pp_dem |> 
  filter(contrast == "Hispanic and/or not white vs non-Hispanic White") |> 
  mutate(contrast = "Hispanic and/or not White vs non-Hispanic White") |> 
   mutate(ci = str_c("[", round(lower, 3), ", ", round(upper, 3), "]"),
         median = as.character(round(median, 3)),
         probability = as.character(sprintf("%.3f", probability))) |> 
  select(contrast, median, ci, probability) |> 
  rename(`Median` = median,
         `Bayesian CI` = ci,
         `Probability` = probability)

pp_geography <- pp_dem |> 
  filter(contrast == "small town/rural vs urban/suburban") |> 
  mutate(ci = str_c("[", round(lower, 3), ", ", round(upper, 3), "]"),
         median = as.character(round(median, 3)),
         probability = as.character(sprintf("%.3f", probability))) |> 
  select(contrast, median, ci, probability) |> 
  rename(`Median` = median,
         `Bayesian CI` = ci,
         `Probability` = probability)

pp_education <- pp_dem |> 
  filter(contrast == "high school or less vs some college") |> 
  mutate(ci = str_c("[", round(lower, 3), ", ", round(upper, 3), "]"),
         median = as.character(round(median, 3)),
         probability = as.character(sprintf("%.3f", probability))) |> 
  select(contrast, median, ci, probability) |> 
  rename(`Median` = median,
         `Bayesian CI` = ci,
         `Probability` = probability)



pp_sex |> 
  bind_rows(pp_income) |> 
  bind_rows(pp_race) |> 
  bind_rows(pp_geography) |> 
   bind_rows(pp_education) |> 
  kbl() |> 
  kable_classic()
```




