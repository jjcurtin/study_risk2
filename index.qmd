---
title: Dynamic lapse risk prediction in a national sample of individuals with opioid use disorder using personal sensing and machine learning
author:
  - name: Kendra Wyant 
    corresponding: false
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    corresponding: true
    email: jjcurtin@wisc.edu
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison 
financial-support: This research was supported by a grant from the National Institute on Drug Abuse (NIDA; R01 DA047315 to John J. Curtin).
keywords:
  - Substance use disorders
  - Precision mental health 
abstract: |
  Enter abstract here. 
date: last-modified
number-sections: false 
editor_options: 
  chunk_output_type: console
format:
  html: default
  docx: default
  apaish-typst: 
    documentmode: man
citeproc: true
bibliography: references.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
---

```{r}
#| echo: false
#| message: false

library(tidyverse)
suppressPackageStartupMessages(source("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true"))

path_models <- format_path(str_c("risk2/models/combined"))
  
pp_perf <- read_csv(here::here(path_models, "pp_perf_tibble_xgboost_v7.csv"),
                    show_col_types = FALSE) 

probs <- read_rds(here::here(path_models, "preds_xgboost_kfold_6_x_5_v7_dyn_dem_oud.rds"))

preds_all_raw <- probs |> 
  mutate(bins = cut(prob_raw, breaks = seq(0, 1, .1)), 
         lapse = if_else(label == "Lapse", 1, 0)) |> 
  count(bins)
```

# Introduction


# Methods

## Transparency and Openness
We adhere to research transparency principles that are crucial for robust and replicable science. First, we published this project's protocol as a pre-registered report during the initial enrollment of pilot participants [@moshontzProspectivePredictionLapses2021]. Second, we followed the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis extension for Artificial Intelligence (TRIPOD+AI) guidelines [@collinsTRIPODAIStatementUpdated2024]. A TRIPOD+AI checklist is made available in this manuscripts supplemental materials. Finally, our data, questionnaires, and other study materials are publicly available on our OSF page ([https://osf.io/zvm7s/overview](https://osf.io/zvm7s/overview)) and our annotated analysis scripts and results are publicly available on our study website ([https://jjcurtin.github.io/study_risk2/](https://jjcurtin.github.io/study_risk2/)).

For transparency, we document changes made to the pre-registered report as follows: 

1. Our candidate models considered features derived from a subset of self-report measures administered at intake, daily EMA, and geolocation sensing data (see Measures section). Consequently, we excluded cellular communication data, daily video check-ins, app-usage data, monthly self-report surveys, and a subset of intake measures. These decisions were informed from other personal sensing work our group has done with alcohol use disorder, technological limitations, and considerations made around balancing the need for a diverse array of features to capture the complex nature of lapse and prioritizing interpretable features for intervention and low burden on the individual providing the data. 

In our work with alcohol use disorder we have found that EMA and geolocation sensing provide moderate to excellent predictive signal [@wyantMachineLearningModels2024; @wyantForecastingRiskAlcoholunderreview<!--claire gps paper-->]. Cellular communications, however, have fallen short of these thresholds [<!--meta paper-->]. Moreover, we were only able to collect cellular communication sensing data from individuals with Android phones due to Apple's strict regulation of app access to communications. Thus, inclusion of these features would result in a model that could only be deployed within an android operating system. Daily video check-ins were not collected from all participants due to technical issues within the A-CHESS app and were therefore excluded. Additionally, app usage data proved to be sparse and unusable. Our app was a research version of a digital therapeutic not optimized to promote continued engagement throughout the study. Self-report measures can dramatically increase burden on the individual providing data. Our monthly surveys were lengthy (20-30 minutes) and in our work with alcohol use disorder we have found no incremental predictive value above sensing data for including these measures. Individual differences in severity of use and stability of recovery at the time of intake has, however, emerged as offering some predictive signal<!--meta paper-->. We also include key demographics that have been shown to be related to substance use treatment access and outcomes.<!--explain why important to add to model. add cites-->

2. In our protocol we proposed to use repeated cross-validation for model selection and a single held-out test set for model evaluation. Despite nearly achieving our goal number of particpants recruited (N=451/480), the number of participants with usable data for our analyses was much lower (N=300). We eliminated the held-out test set to make the most efficient use of our sample (i.e., maximizing the amount of data used for training and evaluation). We believe 6 repeats of cross-validation is sufficient to mitigate concerns of optimization bias and yield generalizable estimates of model performance in new data. 

## Participants


## Procedure
All procedures were approved by the University of Wisconsin-Madison Institutional Review Board (Study #2019-0656). All participants provided written<!--verbal?--> informed consent.


## A-CHESS

## Measures


### Individual Differences


### Ecological Momentary Assessment


### Geolocation Sensing


## Data Analytic Strategy
Data preprocessing, modeling, and Bayesian analyses were done in R using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020; @kuhnTidyposteriorBayesianAnalysis2022; @goodrichRstanarmBayesianApplied2023]. Models were trained and evaluated using high-throughput computing resources provided by the University of Wisconsin Center for High Throughput Computing [@chtc].

### Feature Engineering
Features were engineered from two types of data: 

1. Static individual differences in demographic and OUD characteristics. 

2. Dynamic EMA and geolocation sensing data.

- geolocation - within 50 meters of a known location



We imputed missing data using median imputation for numeric features and mode imputation for nominal features. We selected coarse median/mode methods for handling missing data due to the computational costs associated with more advanced forms of imputation (e.g., KNN imputation, multiple imputation). Importantly, our imputation calculations are done using only held-in data and can be applied to any new observation.
<!--Notes from Ross on proposal about handling missing data: 
- Could always include a separate coding for missing (e.g., -99), which could be informative in and of itself
- Otherwise, true imputation would be better using mice - computationally very simple — defaults are linear models
-->

### Prediction Windows and Labels

- Window starts 6am in participants own timezone (derived from gps)

### Model Selection
Our performance metric for model selection and evaluation was area under the Receiver Operating Characteristic curve (auROC).  <!--expand on choice? [@vancalsterEvaluationPerformanceMeasures2025]-->.

The best model configuration was selected using 6 repeats of participant-grouped 5-fold cross-validation.<!--cite and define grouped cross validation--> Folds were stratified by a between-subject measure of our outcome (no lapse vs. any lapse). 

Model configurations differed on: 

1. Statistical algorithm (and hyperparameter tuning)

2. Resampling of minority outcome. All resampling was exclusively done with only held-in training data to prevent biasing performance estimates [@vandewieleOverlyOptimisticPrediction2021]. 

We repeated the above process to select a best baseline model that was limited to features derived from individual differences in demographics and OUD characteristics. 


### Model Evaluation
We used a Bayesian hierarchical generalized linear model to estimate the posterior distribution and 95% credible intervals (CI) for auROC for the 30 held-out test sets for our best performing model. We used weakly informative, data-dependent priors to regularize and reduce overfitting. Random intercepts were included for repeat and fold (nested within repeat).

- Model contrast

Using the same 30 held-out test sets, we calculated the median posterior probability and 95% Bayesian CI for auROC for each model separately by gender (not male vs. male), race/ethnicity (Hispanic and/or non-White vs. non-Hispanic White), income (below poverty line vs. above poverty line), geographic location (small town/rural vs. urban/suburban), and education (high school or less vs. some college). We conducted Bayesian group comparisons to assess the likelihood that each model performs differently by group.


### Calibration
We calibrated our probabilities using Platt scaling. We calculated Brier scores to assess the accuracy of our raw and calibrated probabilities for our best model. Brier scores range from 0 (perfect accuracy) to 1 (perfect inaccuracy). We provide calibration plots for the raw and calibrated probabilities in the manuscript and histograms of predicted probabilities by true lapse outcome in the supplement.

### Feature Importance

# Results

## Participants
We recruited 451 participants across 47 states in the United States from April 2021 through December 2024. A total of 336 participants were eligible, consented, and remained on study for at least one month. Of these, 11 participants were excluded due to unusually low adherence^[One participant completed only 3 EMA prompts over 88 days and 10 participants had fewer than 20 geolocation points per day on average.], 13 participants were excluded due to insufficient context data for geolocation points^[We required participants have at least two contextualized locations other than their home.], 1 participant was excluded due to geolocation data indicating they were not residing in the United States, 10 participants were excluded due to evidence of careless responding on EMAs. Our final analysis sample consisted of 300 participants. Participant demographic and OUD characteristics is presented in @tbl-1.

{{< embed notebooks/mak_tables.qmd#tbl-1 >}}

## Adherence, Features, and Labels
Mean days on study across participants was 298 days (range 32-395 days). 83% of participants (250/300) remained on study for at least six months. EMA adherence was high. On average participants completed 73% of the daily EMA prompts (range 24-100%). Participants provided, on average, 313 daily geolocation points (range 20-825). 

Our final feature set consisted of 640 features<!--recheck-->. The proportion of missing values across features was low (mean=.03, range = 0-.12).<!--recheck--> Across participants we generated a total of 88,973 day-level labels.<!--summarize number of observations and participants in training data--> Forty percent of participants (119/300) reported an opioid lapse while on study (mean=5.73, range 0-117). This resulted in 1.93% of the labels positive for lapse (1,720/88,973 labels). 


## Performance
The best model configuration used an xgboost statistical algorithm and up-sampled the minority class<!--recheck-->.^[The best model configuration used 1:4 upsampling of the minority class and the following hyperparameter values: learning rate = .1, tree depth = 1, mtry = 50.]<!--added this footnote to follow TRIPOD-AI checklist--> The median posterior probability for the best model was `r round(pp_perf$pp_median[1], 2)`, with narrow 95% CI ([`r round(pp_perf$pp_lower[1], 2)`, `r round(pp_perf$pp_upper[1], 2)`]). 

We compared our best model's performance to a baseline model that only used information gathered at intake (i.e., individual differences in demographic and OUD characteristics) to evaluate the incremental predictive value of adding dynamic features. The median posterior probability for the baseline model was  `r round(pp_perf$pp_median[2], 2)` (95% CI [`r round(pp_perf$pp_lower[2], 2)`, `r round(pp_perf$pp_upper[2], 2)`]). A Bayesian model comparison revealed extremely strong evidence that the best model was more predictive than the baseline model (probability = 1.00).

Our fairness subgroup comparisons revealed no evidence that performance meaningfully differed by geographic location (probability = 0.68), weak evidence that performance differed by education (probability = 0.83), and strong evidence that performance differed by gender, income, and race and ethnicity (probabilities > 0.99). Notably, our model performed better for individuals with an annual income below the federal poverty line compared to individuals above the federal poverty line, thus favoring the disadvantaged group. While differences in performance estimates exist across subgroups, they are not likely clinically meaningful as all of our subgroups yielded median auROCs between 0.91 - 0.94 (@fig-1). A table of fairness subgroup comparisons is available in the supplement.

{{< embed notebooks/mak_figures.qmd#fig-1 >}}


## Calibration
Our raw model predicted probabilities were generally well-calibrated (brier score = `r round(yardstick::brier_class(probs, truth = label, prob_raw)$.estimate, 3)`). We attempted to improve calibration with Platt scaling but results were comparable (brier score = `r round(yardstick::brier_class(probs, truth = label, prob_logi)$.estimate, 3)`). Calibration plots for the raw uncalibrated probabilities and calibrated probabilities with Platt scaline are presented in @fig-2. Histograms of predicted probabilities by true lapse outcome are available in the supplement.   

{{< embed notebooks/mak_figures.qmd#fig-2 >}}

## Feature Importance

For brevity, we are only displaying the top 30 features, as defined by the 30 largest absolute mean shapley values. In the supplement, we provide full figures of all possible feature categories.<!--number of categories-->

{{< embed notebooks/mak_figures.qmd#fig-3 >}}

{{< embed notebooks/mak_figures.qmd#fig-4 >}}


# Discussion

- Discuss contrast between top global features and "actual" important features (i.e., what comes up day to day as being important). What does this mean for how we traditionally conceptualize feature importance?

- When discussing fairness - we don’t look at age. People over 65 is an important group to look at but they were not in our sample

- Decision not include misclassification cost analyses: acknowledge that discrimination and calibration don't equate to clinical utilility. Decision analysis tools, such as net benefit curves could be used to quantify clinical benefit at relevant probability thresholds. Our model is designed to not be used to make decisions about whether or not to treat but to provide model feedback to individuals each day to help them monitor their risk for lapse. Therefore the decision has already been made to always treat. Should these models be used to inform when to deploy more cost-intensive interventions (e.g., communicating risk level to a therapist to initiate contact) these analyses will be important next steps.



# References
::: {#refs}
:::
