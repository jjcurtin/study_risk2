---
title: Using smartphone sensing and machine learning for personalized daily lapse risk prediction in a national sample of people with opioid use disorder
author:
  - name: Kendra Wyant 
    corresponding: false
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    corresponding: true
    email: jjcurtin@wisc.edu
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison 
financial-support: This research was supported by a grant from the National Institute on Drug Abuse (NIDA; R01 DA047315 to John J. Curtin).
keywords:
  - Substance use disorders
  - Precision mental health 
abstract: |
  Enter abstract here. 
date: last-modified
number-sections: false 
editor_options: 
  chunk_output_type: console
format:
  html: default
  docx: default
  apaish-typst: 
    documentmode: man
citeproc: true
bibliography: references.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
---

```{r}
#| echo: false
#| message: false

library(tidyverse)
suppressPackageStartupMessages(source("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true"))

path_models <- format_path(str_c("risk2/models/combined"))
  
pp_perf <- read_csv(here::here(path_models, "pp_perf_tibble_xgboost_v7.csv"),
                    show_col_types = FALSE) 

probs <- read_rds(here::here(path_models, "preds_xgboost_kfold_6_x_5_v7_dyn_dem_oud.rds"))

preds_all_raw <- probs |> 
  mutate(bins = cut(prob_raw, breaks = seq(0, 1, .1)), 
         lapse = if_else(label == "Lapse", 1, 0)) |> 
  count(bins)
```

# Introduction


# Methods

## Transparency and Openness
We adhere to research transparency principles that are crucial for robust and replicable science. First, we published this study's protocol as a registered report (International Registered Report Identifier [IRRID]: DERR1-10.2196/29563) during the initial enrollment of pilot participants [@moshontzProspectivePredictionLapses2021]. Second, we followed the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis extension for Artificial Intelligence (TRIPOD+AI) guidelines [@collinsTRIPODAIStatementUpdated2024]. A TRIPOD+AI checklist is available in the supplement.<!--checklist not there yet--> Finally, our features, labels, questionnaires, and other study materials are publicly available on our OSF page ([https://osf.io/zvm7s/overview](https://osf.io/zvm7s/overview)) and our annotated analysis scripts and results are publicly available on our study website ([https://jjcurtin.github.io/study_risk2/](https://jjcurtin.github.io/study_risk2/)).

<!--We could probably put this part below in the supplement-->
For transparency, we documented the changes made to the registered report below: 

### Sensing Data Streams 
Our candidate models included features derived from a subset of intake self-report measures, daily EMA, and geolocation sensing data (see Measures section). Consequently, we excluded cellular communication data, daily video check-ins, app usage data, and certain self-report measures. These decisions were informed by (1) our group's personal sensing work with alcohol use disorder, (2) technological constraints, and (3) the desire to balance feature diversity for capturing lapse complexity and minimizing participant burden and computational cost. 

1. In our alcohol use disorder research, EMA and geolocation sensing have shown moderate to excellent predictive signal (aurocs .72-.91<!--confirm claire auroc-->) [@wyantMachineLearningModels2024; @wyantForecastingRiskAlcoholunderreview<!--Cite claire gps paper-->]. Cellular communication sensing, however, has fallen short of these thresholds<!--meta paper-->. Moreover, Apple places strict restrictions on app access to communications, meaning inclusion of these features would result in a model that could only be deployed within an Android operating system. 
    
2. We discontinued collecting daily video check-ins about 6 months into the 2.5 year data collection due to technical issues and App usage data (beyond the required study tasks) were generally sparse and inconsistent across participants. 
3. Self-report measures can substantially increase data collection burden and expand the feature space. Therefore, we opted to not include all measures. Monthly surveys were lengthy (20–30 minutes) and, based on our work with alcohol use disorder, added no incremental predictive value beyond the dynamic sensing data. However, individual differences in severity of use and stability of recovery at the time of intake have demonstrated some predictive signal. Key demographic variables known to influence OUD treatment access and clinical outcomes are also important to reduce model bias by preventing these effects being encoded into the model indirectly by proxy variables. See Measures section for a comprehensive list of all retained self-report items.

### Resampling Method
Our initial protocol proposed repeated cross-validation for model selection and a single held-out test set for evaluation. Although we nearly reached our recruitment goal (N = 451/480), the number of participants with usable data was much lower (N = 300). To maximize data use, we eliminated the held-out test set and relied on repetitions in our cross-validation to mitigate optimization bias and provide generalizable performance estimates (i.e., 6 repeats of 5-fold cross-validation to generate 30 held-out test sets).

## Participants
We recruited 451 participants in early recovery from opioid use disorder from across the United States. We recruited through national digital advertising and collaborations with treatment providers at MOUD clinics. Our recruitment strategy was designed to create a diverse sample with respect to demographics (gender, age, race, and ethnicity), and geographic location (urban and rural). We required participants:

- were age 18 or older,
- could read, write, and speak in English,
- were enrolled in and adherent with an MOUD program for at least 1 month but no longer than 12 months, and
- had an Android smartphone with an active cellular plan.  

We did not exclude participants for comorbid substance use or other psychiatric disorders. Participants were compensated up to \$75 per month for completing study tasks (i.e., EMAs, monthly surveys and sharing sensing data) and were paid \$50 per month to offset the cost of maintaining a cellphone plan. 

## Procedure
Participants completed three video or phone visits over approximately 12 months. During the enrollment visit, study staff obtained written informed consent and collected demographic information. They then walked participants through how to download the Smart Technology for Addiction Recovery (STAR) study app, provided a set of video tutorials to learn how to use the app, and instructed participants to complete the intake survey within the app. The STAR app was developed with the UW Madison Center for Health Enhancement Systems Studies (CHESS) and used for all data collection. Within the app participants could control their data sharing options, monitor completed study tasks, receive reminder notifications about tasks, message staff, and access CHESS's suite of resources and tools for people in recovery from AUD. Enrolled participants met with study staff 1 week later to troubleshoot technical issues. At the end of the study enrollment period participants met briefly with study staff for a debriefing session. While on study, participants were expected to complete daily EMA, monthly surveys, and share geolocation sensing data. Other sensing data streams (i.e., daily video check-ins, cellular communications, and app usage data) were collected as part of the parent grant's aims (NIDA R01 DA047315). All procedures were approved by the University of Wisconsin-Madison Institutional Review Board (Study #2019-0656). 


## Measures

### Individual Differences
We collected self-report information about demographics (age, gender, orientation, race, ethnicity, education, and income). Zip codes from participants' reported home addresses were matched to Rural-Urban Commuting Area (RUCA) codes to identify whether individuals resided in rural or urban areas <!--cite[@RuralUrbanCommutingArea]-->. We followed guidelines from the United States Health Resources and Services Administration and define urban as an area where the primary commuting flow is within a metropolitan core of 50,000 or more people (RUCA code = 1) and rural as anything not urban (RUCA codes 2-10)<!--cite [@HowWeDefine]-->.   

We also collected information about OUD history to characterize OUD severity and recovery stability, including self-reported Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition OUD symptoms [@americanpsychiatricassociationDiagnosticStatisticalManual2022], recovery satisfaction, motivation, and confidence, whether they intended to avoid using other drugs, perceived efficacy and likelihood of adhering to their MOUD medication [@moriskyConcurrentPredictiveValidity1986], past month opioid use, past month residential treatment for OUD, past month receipt of psychiatric medication, frequency of counseling sessions and self-help meetings in the past month, preferred opioid, preferred route of administration, and lifetime history of overdose. As part of the aims of the parent project, we collected many other trait and state measures throughout the study. A complete list of all measures can be found in our registered report [@moshontzProspectivePredictionLapses2021].

### Ecological Momentary Assessment
Participants completed a brief (1-2 minute) EMA each day on study through the STAR app. The EMA became available in the app at 5:00 AM CST each morning and participants had 24 hours to complete it. Participants could enable push notifications for reminder prompts to complete the assessment. Each EMA had 16-items that asked participants to report the date and time of any recent opioid use for nonmedical reasons not yet reported. These reports served as the primary outcome for the lapse risk prediction model. Participants also reported any other drugs that they had used in the past 24 hours by selecting all that apply from a list of substance categories. Other items asked about mood, pain, sleep quality, urges to use opioids, risky situations, stressful and pleasant events, and MOUD adherence in the past 24 hours. The EMA concluded with 2 future-facing items that asked about participants’ motivation and confidence to continue to avoid using opioids for nonmedical reasons over the next week. The full EMA questionnaire is available in the supplement.<!--need to add-->

### Geolocation Sensing
The STAR app passively recorded participants’ time-stamped geolocations (i.e., latitude and longitude) every 1.5-15 minutes, depending on their movement. We augmented the geolocation data with self-reported subjective contexts. On each monthly survey we asked a set of 6 questions about frequently visited locations (i.e., spending more than 3 minutes at a location 2 or more times in a month) from the previous month. Participants were asked to describe the type of place, what they typically do there, the general frequency of pleasant and unpleasant experiences associated with the place, and the extent to which spending time there supports or undermines their recovery. 

## Data Analytic Strategy
Data preprocessing, modeling, and Bayesian analyses were done in R using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020; @kuhnTidyposteriorBayesianAnalysis2022; @goodrichRstanarmBayesianApplied2023]. Models were trained and evaluated using high-throughput computing resources provided by the University of Wisconsin Center for High Throughput Computing [@chtc].

### Feature Engineering
Features were calculated using only data collected before the start of each prediction window to ensure our models were making true future predictions. Features were engineered from three sources: 

1. *The prediction window.* We created dummy-coded features for day of the week for the start of the prediction window.

2. *Static individual differences in demographic and OUD characteristics.* For demographics, we created ordinal features for age, education, and income based on predefined response ranges (see @tbl-1 for ranges), an ordinal feature for RUCA code associated with home address (range 1-10), and dummy coded features for gender (male vs. not male) and race and ethnicity (non-Hispanic White vs. Hispanic and/or not White). For OUD characteristics we created ordinal features for recovery satisfaction, recovery motivation, recovery confidence, MOUD side effects experienced, perceived efficacy of MOUD medication, and likelihood of continuing MOUD (ranges 0-4), ordinal features for frequency of counseling sessions and self-help meetings attended in the past month (ranges 0-3), a quantitative feature for number of self-reported DSM-5 OUD symptoms, an ordinal feature for lifetime history of overdose (range 0-4), and dummy coded features for past month opioid use (yes vs. no), past month detox or residential treatment (yes vs. no), past month psychiatric medication (yes vs. no), preferred opioid (fentanyl vs. heroin vs. prescription opioid not for opioid treatment vs. medication for opioid treatment), and preferred route of administration (injection vs. oral vs. smoke vs. sniff/snort vs. other).  

3. *Dynamic EMA and geolocation sensing data.* For both sets of dynamic features we calculated two types of features: raw and difference features. Raw features represent the feature value calculated within a scoring epoch (e.g., the maximum urge rating reported on EMA during the 48 hours immediately preceding the start of the prediction window). Difference features capture participant-level changes from their baseline scores. Specifically, we subtracted each participant’s mean score for each feature (using all available data prior to the prediction window) from the associated raw feature (e.g., the participant’s average urge rating across all prior EMAs subtracted from the maximum urge rating in the preceding 48 hours).

    We used three scoring epochs (48, 72, and 168 hours before the start of the prediction window) to create features from the daily EMA. We calculated raw and difference features for min, max, median, and most recent responses for the 13 5-point likert scale items (items 4-16; pain, urge, risky situation, stressful event, pleasant event, sleep, depressed, angry, anxious, relaxed, happy, abstinence motivation, and abstinence confidence) across all EMAs in each epoch for a given participant. We also calculated raw and difference rate features based on counts of opioid lapses, other drug use, missed MOUD doses (items 1-3), and completed EMAs across all EMAs in each scoring epoch.

    We used six scoring epochs (6, 12, 24, 48, 72, and 168 hours before the start of the prediction window) to create features from the densely sampled geolocation data. Raw geolocation points were cross-checked against known locations with reported subjective context. We used a threshold of 50 meters for matching context to geolocation points. We calulated raw and difference features for sum duration of time spent in transit (i.e., moving faster than 4 miles per hour) and time out in the evenings (i.e., not at their home between the hours of 7:00pm-4:00am). We calulated raw and difference features for sum duration of time spent at locations according to what they indicated they do at the location (spend time with friends, socialize with new people, religious activities, relax, spend time with family, volunteer, receive mental health care, receive physical health care, receive MOUD treatment, drink alcohol, take classes, work), how pleasant and unpleasant their experiences typically are at the location (Always, Most of the time, Sometimes, Rarely, Never), and how helpful and harmful the location is to their recovery (Extremely, Considerably, Moderately, Mildly, Not at all). We also calculated a raw and difference features of location variance (i.e., the extent to which a participant's location changes over a scoring epoch).

All features (sets 1-3 above) were included in our main model. We also fit a baseline model that excluded the dynamic EMA and geolocation features (i.e., only using feature sets 1 and 2 above) to assess the incremental predictive value of these sensing features. 

Other feature engineering steps performed during cross-validation included imputing missing values (median imputation for numeric features and mode imputation for nominal features), dummy coding nominal features, normalizing features to have a mean of zero and standard deviation of 1, bringing outlying values (|z-score| > 5) to the fence, and removing zero and near-zero variance features as determined from held-in data. We selected coarse median/mode methods for handling missing data due to the low rates of missing values and computational costs associated with more advanced forms of imputation (e.g., KNN imputation, multiple imputation). A sample feature engineering script (i.e., tidymodels recipe) containing all feature engineering steps is available on our OSF study page.

### Lapse Labels
Prediction windows started at 6:00am in participants' local timezones and ended at 5:59am the next day. This window start time was selected to match a typical wake-sleep cycle as opposed to midnight-to-midnight calendar day. For each participant the first prediction window began at 6:00am on their second day of participation and rolled forward day-by-day until their participation ended (i.e., the last prediction window ended at 5:59am on the day of their last recorded EMA). 

Participants reported lapses on the first EMA item. If participants responded "yes" to the question "Have you used any opioids for non-medical reasons that you have not yet reported?", they were prompted to select the day(s) and time(s) of the lapse(s). Times were reported in 6-hour increments (12:00am–5:59am, 6:00am–11:59am, 12:00pm–5:59pm, 6:00pm–11:59pm). These responses were used to label each prediction window as lapse or no lapse. For example, if a participant reported a lapse on December 12 from 12:00am-5:59am the prediction window spanning 6:00am on December 11 through 5:59am on December 12 was labeled as a lapse.

### Model Configurations

Model configurations differed on: 

1. Statistical algorithm (and hyperparameter tuning)

2. Resampling of minority outcome. All resampling was exclusively done with only held-in training data to prevent biasing performance estimates [@vandewieleOverlyOptimisticPrediction2021]. 

3. Baseline vs. dynamic feature sets

The best model configuration was selected using 6 repeats of participant-grouped 5-fold cross-validation. Grouped cross-validation assigns all data from a participant as either held-in or held-out to avoid bias introduced when predicting a participant’s data from their own data. Folds were stratified by a between-subject measure of our outcome (no lapse vs. any lapse). 


### Model Evaluation
We evaluate the best model’s probability predictions across three domains: discrimination, calibration, and overall performance. We follow best recommendations for reporting measures and plots to characterize these performance domains [@vancalsterEvaluationPerformanceMeasures2025]. Classification and clinical utility are two other important domains for evaluating a model intended to make or inform a decision (e.g., whether to send a support message to an individual). In our scenario there is no decision to be made. Hypothetically, every day an individual would receive a message regardless of lapse risk, equivalent to a treat all condition. Therefore, we have constrained our evaluation to focus on the probability estimates for evaluating performance. 

#### auROC and Model Comparisons
Our performance metric for model selection and evaluation was the area under the receiver operating characteristic curve (auROC). auROC represents the probability that the model will assign a higher predicted probability to a randomly selected positive case (lapse) compared to a randomly selected negative case (no lapse). auROC is an aggregate measure of discrimination across all possible decision thresholds. This is important because optimal thresholds depend on the setting, outcome prevalence, and the relative costs of misclassification, and should be addressed separately (e.g., with a decision curve analysis).

We used a Bayesian hierarchical generalized linear model to estimate the posterior distribution and 95% credible intervals (CI) for auROC for the 30 held-out test sets for our best performing model. We used weakly informative, data-dependent priors to regularize and reduce overfitting. Random intercepts were included for repeat and fold (nested within repeat).

- Model contrast

Using the same 30 held-out test sets, we calculated the median posterior probability and 95% Bayesian CI for auROC for each model separately by gender (not male vs. male), race/ethnicity (Hispanic and/or non-White vs. non-Hispanic White), income (below poverty line vs. above poverty line), geographic location (rural vs. urban), and education (high school or less vs. some college). We conducted Bayesian group comparisons to assess the likelihood that each model performs differently by group.

- Gender, race/ethnicity, education, income, and geographic location are used for our fariness subgroup analyses. As defined above we defined urban as RUCA code of 1 and rural as RUCA codes 2-10. Given that there is not a clear consensus on delineating urban vs. rural, however, we also report a fairness model comparison for urban as RUCA codes 1-2 and urban as RUCA coddes 3-10 in the supplement.


#### Calibration and Overall Performance
Calibration is an indicator of how well a model's predicted probabilities correspond to the true observed outcomes. For example, a well-calibrated model that assigns a 30% lapse risk prediction should observe lapses in approximately 30% of such cases. We used Platt scaling to calibrate our raw probabilities [@plattProbabilisticOutputsSupport1999]. We provided a calibration plot of the raw and calibrated probabilities. To characterize overall performance we reported Brier scores, which range from 0 (perfect accuracy) to 1 (perfect inaccuracy), for the raw and calibrated probabilities. We also provided histograms of risk probability distributions by true lapse outcome.

### Feature Importance

# Results

## Participants
We recruited 451 participants across 47 states in the United States from April 2021 through December 2024. A total of 336 participants were eligible, consented, and remained on study for at least one month. Of these, 11 participants were excluded due to unusually low adherence^[One participant completed only 3 EMA prompts over 88 days and 10 participants had fewer than 20 geolocation points per day on average.], 13 participants were excluded due to insufficient context data for geolocation points^[We required participants have at least two contextualized locations other than their home.], 1 participant was excluded due to geolocation data indicating they were not residing in the United States, 10 participants were excluded due to evidence of careless responding on EMAs. Our final analysis sample consisted of 300 participants. Participant demographic and OUD characteristics is presented in @tbl-1.

{{< embed notebooks/mak_tables.qmd#tbl-1 >}}

## Adherence, Features, and Labels
Mean days on study across participants was 298 days (range 32-395 days). 83% of participants (250/300) remained on study for at least six months. EMA adherence was high. On average participants completed 73% of the daily EMA prompts (range 24-100%). Participants provided, on average, 313 daily geolocation points (range 20-825). 

Our final feature set consisted of 640 features<!--recheck-->. The proportion of missing values across features was low (mean=.03, range = 0-.12).<!--recheck--> Across participants we generated a total of 88,973 day-level labels.<!--summarize number of observations and participants in training data--> Forty percent of participants (119/300) reported an opioid lapse while on study (mean=5.73, range 0-117). This resulted in 1.93% of the labels positive for lapse (1,720/88,973 labels). 


## auROC and Model Comparisons
The best model configuration used an xgboost statistical algorithm and up-sampled the minority class<!--recheck-->.^[The best model configuration used 1:4 upsampling of the minority class and the following hyperparameter values: learning rate = .1, tree depth = 1, mtry = 50.]<!--added this footnote to follow TRIPOD-AI checklist--> The median posterior probability for the best model was `r round(pp_perf$pp_median[1], 2)`, with narrow 95% CI ([`r round(pp_perf$pp_lower[1], 2)`, `r round(pp_perf$pp_upper[1], 2)`]). 

We compared our best model's performance to a baseline model that only used information gathered at intake (i.e., individual differences in demographic and OUD characteristics) to evaluate the incremental predictive value of adding dynamic features. The median posterior probability for the baseline model was  `r round(pp_perf$pp_median[2], 2)` (95% CI [`r round(pp_perf$pp_lower[2], 2)`, `r round(pp_perf$pp_upper[2], 2)`]). A Bayesian model comparison revealed extremely strong evidence that the best model was more predictive than the baseline model (probability = 1.00).

Our fairness subgroup comparisons revealed no evidence that performance meaningfully differed by geographic location (probability = 0.68), weak evidence that performance differed by education (probability = 0.83), and strong evidence that performance differed by gender, income, and race and ethnicity (probabilities > 0.99). Notably, our model performed better for individuals with an annual income below the federal poverty line compared to individuals above the federal poverty line, thus favoring the disadvantaged group. While differences in performance estimates exist across subgroups, they are not likely clinically meaningful as all of our subgroups yielded median auROCs between 0.91 - 0.94 (@fig-1). A table of fairness subgroup comparisons is available in the supplement.

{{< embed notebooks/mak_figures.qmd#fig-1 >}}

<!--update fig to use rural vs. urban labels-->

## Calibration and Overall Performance
Our raw model predicted probabilities were generally well-calibrated (brier score = `r round(yardstick::brier_class(probs, truth = label, prob_raw)$.estimate, 3)`) indicating that predicted probabilities generally matched the true observed lapse rates. We attempted to improve calibration with Platt scaling but results were comparable (brier score = `r round(yardstick::brier_class(probs, truth = label, prob_logi)$.estimate, 3)`). Calibration plots for the raw uncalibrated probabilities and calibrated probabilities with Platt scaline are presented in @fig-2. Histograms of predicted probabilities by true lapse outcome are available in the supplement.   

{{< embed notebooks/mak_figures.qmd#fig-2 >}}

## Feature Importance

For brevity, we are only displaying the top 30 features, as defined by the 30 largest absolute mean shapley values. In the supplement, we provide full figures of all possible feature categories.<!--number of categories-->

{{< embed notebooks/mak_figures.qmd#fig-3 >}}

{{< embed notebooks/mak_figures.qmd#fig-4 >}}


# Discussion

- Discuss contrast between top global features and "actual" important features (i.e., what comes up day to day as being important). What does this mean for how we traditionally conceptualize feature importance?

- When discussing fairness - we don’t look at age. People over 65 is an important group to look at but they were not in our sample

- Decision not include misclassification cost analyses: acknowledge that discrimination and calibration don't equate to clinical utilility. Decision analysis tools, such as net benefit curves could be used to quantify clinical benefit at relevant probability thresholds. Our model is designed to not be used to make decisions about whether or not to treat but to provide model feedback to individuals each day to help them monitor their risk for lapse. Therefore the decision has already been made to always treat. Should these models be used to inform when to deploy more cost-intensive interventions (e.g., communicating risk level to a therapist to initiate contact) these analyses will be important next steps.



# References
::: {#refs}
:::
