---
title: Dynamic lapse risk prediction in a national sample of individuals with opioid use disorder using personal sensing and machine learning
author:
  - name: Kendra Wyant 
    corresponding: false
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    corresponding: true
    email: jjcurtin@wisc.edu
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison 
financial-support: This research was supported by a grant from the National Institute on Drug Abuse (NIDA; R01 DA047315 to John J. Curtin).
keywords:
  - Substance use disorders
  - Precision mental health 
abstract: |
  Enter abstract here. 
date: last-modified
number-sections: false 
editor_options: 
  chunk_output_type: console
format:
  html: default
  docx: default
  apaish-typst: 
    documentmode: man
citeproc: true
bibliography: references.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
---

```{r}
#| echo: false
#| message: false

library(tidyverse)
suppressPackageStartupMessages(source("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true"))

path_models <- format_path(str_c("risk2/models/combined"))
  
pp_perf <- read_csv(here::here(path_models, "pp_perf_tibble_xgboost_v7.csv"),
                    show_col_types = FALSE) 

probs <- read_rds(here::here(path_models, "preds_xgboost_kfold_6_x_5_v7_dyn_dem_oud.rds"))

preds_all_raw <- probs |> 
  mutate(bins = cut(prob_raw, breaks = seq(0, 1, .1)), 
         lapse = if_else(label == "Lapse", 1, 0)) |> 
  count(bins)
```

# Introduction


# Methods

<!--- Notes:
N = 301 (required 2 known locations other than home, required at least 20 gps points per day on average)
- Window starts 6am in participants own timezone (derived from gps)
- 6 x 5 kfold
- Pre registered report
- Considered model configurations with features from 3 different sources
-->

## Transparency and Openness
We adhere to research transparency principles that are crucial for robust and replicable science. We reported how we determined the sample size, all data exclusions, all manipulations, and all study measures. Our data, questionnaires, and other study materials are publicly available on our OSF page ([https://osf.io/zvm7s/overview](https://osf.io/zvm7s/overview)), and our annotated analysis scripts and results are publicly available on our study website ([https://jjcurtin.github.io/study_risk2/](https://jjcurtin.github.io/study_risk2/)).


-TRIPOD+AI checklist [@collinsTRIPODAIStatementUpdated2024]

## Participants


## Procedure
All procedures were approved by the University of Wisconsin-Madison Institutional Review Board <!--(Study #2015-0780).--> All participants provided written informed consent.



## Measures


### Individual Characteristics


### Ecological Momentary Assessment


## Geolocation Sensing


## Data Analytic Strategy
Data preprocessing, modeling, and Bayesian analyses were done in R using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020; @kuhnTidyposteriorBayesianAnalysis2022; @goodrichRstanarmBayesianApplied2023]. Models were trained and evaluated using high-throughput computing resources provided by the University of Wisconsin Center for High Throughput Computing [@chtc].

### Model Selection
The best model configuration was selected using 6 repeats of participant-grouped 5-fold cross-validation. Folds were stratified by a between-subject measure of our outcome (no lapse vs. any lapse). Model configurations differed on: 

1. Statistical algorithm (and hyperparameter tuning)

2. Feature set

3. Resampling of minority outcome

Features had missing values if the participant did not respond to the relevant EMA question during the associated scoring epoch. The proportion of missing values across features was low <!--(median = .02, range = 0 - .13)-->. We imputed missing data using median imputation for numeric features and mode imputation for nominal features. We selected coarse median/mode methods for handling missing data due to the computational costs associated with more advanced forms of imputation (e.g., KNN imputation, multiple imputation). Importantly, our imputation calculations are done using only held-in data and can be applied to any new observation.


### Model Evaluation
We used a Bayesian hierarchical generalized linear model to estimate the posterior distribution and 95% credible intervals (CI) for the 30 held-out test sets for our best performing model. We used weakly informative, data-dependent priors to regularize and reduce overfitting. Random intercepts were included for repeat and fold (nested within repeat).

Using the same 30 held-out test sets, we calculated the median posterior probability and 95% Bayesian CI for auROC for each model separately by gender (not male vs. male), race/ethnicity (Hispanic and/or non-White vs. non-Hispanic White), income (below poverty line vs. above poverty line), geographic location (small town/rural vs. urban/suburban), and education (high school or less vs. some college). We conducted Bayesian group comparisons to assess the likelihood that each model performs differently by group.

- following best practices for model reporting [@vancalsterEvaluationPerformanceMeasures2025]

### Calibration
We calibrated our probabilities using Platt scaling. We calculated Brier scores to assess the accuracy of our raw and calibrated probabilities for our best model. Brier scores range from 0 (perfect accuracy) to 1 (perfect inaccuracy). We also provide calibration plots for the raw and calibrated probabilities.

### Feature Importance

# Results

## Participants
Data were collected from April 2021 through December 2024. A total of 336 participants were eligible, consented, and remained on study for at least one month. Our final analusis sample consisted of 301 participants. Ten participants were excluded due to unusually low adherence, 13 participants were excluded due to insufficient context data for gps points, 4 participants were excluded due to frequent lapses suggesting they no longer had a goal of abstinence, 8 participants were excluded due to suspected fraud and careless responding. A summary of participant demographic and clinical characteristics is presented in @tbl-1.

{{< embed notebooks/mak_figures.qmd#fig-1 >}}

## Adherence and Labels
Median days on study across participants was 356 days (range 32-395 days). 83% of participants remained on study for at least six months. EMA adherence was high. On average participants completed 73% of the daily EMA prompts (range 24-100%). Participants provided on average <!--X gps points daily (range 20-X).--> 

Across participants we generated a total of 89,125 labels. Thirty-nine percent of participants reported an opioid lapse while on study (mean=5.34, range 0-76). This resulted in 1.80% of the labels positive for lapse (1,608/89,125 labels). 


## Performance
The best model configuration was a down-sampled xgboost statistical algorithm that used only the dynamic feature set (i.e., features from EMA and geolocation). The median posterior probability for the best model was `r round(pp_perf$pp_median[1], 2)`, with narrow 95% CI ([`r round(pp_perf$pp_lower[1], 2)`, `r round(pp_perf$pp_upper[1], 2)`]). 

We compared our best model's performance to a baseline model that only used information gathered at baseline (demographics and individual differences in OUD characteristics) to evaluate the predictive value of adding dynamic features. The median posterior probability for the baseline model was  `r round(pp_perf$pp_median[2], 2)` (95% CI [`r round(pp_perf$pp_lower[2], 2)`, `r round(pp_perf$pp_upper[2], 2)`]). A Bayesian model comparison revealed extremely strong evidence that the best model was more predictive than the baseline model (probability = 1.00).

Our fairness subgroup comparisons revealed no evidence that performance meaningfully differed by geographic location (probability = 0.68), weak evidence that performance differed by education (probability = 0.83), and strong evidence that performance differed by gender, income, and race and ethnicity (probabilities > 0.99). Notably, our model performed better for individuals with an annual income below the federal poverty line compared to individuals above the federal poverty line, thus favoring the disadvantaged group. While differences in performance estimates exist across subgroups, they are not likely clinically meaningful as all of our subgroups yielded median auROCs between 0.91 - 0.94 (@fig-1). A table of fairness subgroup comparisons is available in the supplement.

{{< embed notebooks/mak_figures.qmd#fig-1 >}}


## Calibration
Our raw model predicted probabilities were generally well-calibrated (brier score = `r round(yardstick::brier_class(probs, truth = label, prob_raw)$.estimate, 3)`). We attempted to improve calibration with Platt scaling but results were comparable (brier score = `r round(yardstick::brier_class(probs, truth = label, prob_logi)$.estimate, 3)`). Calibration plots for the raw uncalibrated probabilities and calibrated probabilities with Platt scaline are presented in @fig-cal. We provide histograms for the predicted probabilities of lapse for all observations separately by true outcome in the supplement.   

{{< embed notebooks/mak_figures.qmd#fig-2 >}}

## Feature Importance

<!-- Reference supplemental figures with all feature categories -->

{{< embed notebooks/mak_figures.qmd#fig-5 >}}

{{< embed notebooks/mak_figures.qmd#fig-6 >}}

# Discussion
<!--Discussion notes:
- donâ€™t have people have over 65 important group to look at but not in our sample -->


# References
::: {#refs}
:::
