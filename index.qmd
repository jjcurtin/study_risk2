---
title: Dynamic lapse risk prediction in a national sample of individuals with opioid use disorder using personal sensing and machine learning
author:
  - name: Kendra Wyant 
    corresponding: false
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    corresponding: true
    email: jjcurtin@wisc.edu
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison 
financial-support: This research was supported by a grant from the National Institute on Drug Abuse (NIDA; R01 DA047315 to John J. Curtin).
keywords:
  - Substance use disorders
  - Precision mental health 
abstract: |
  Enter abstract here. 
date: last-modified
number-sections: false 
editor_options: 
  chunk_output_type: console
format:
  html: default
  docx: default
  apaish-typst: 
    documentmode: man
citeproc: true
bibliography: references.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
---

```{r}
#| echo: false
#| message: false

library(tidyverse)
suppressPackageStartupMessages(source("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true"))

path_models <- format_path(str_c("risk2/models/combined"))
  
pp_perf <- read_csv(here::here(path_models, "pp_perf_tibble_xgboost_v7.csv"),
                    show_col_types = FALSE) 

probs <- read_rds(here::here(path_models, "preds_xgboost_kfold_6_x_5_v7_dyn.rds"))

preds_all_raw <- probs |> 
  mutate(bins = cut(prob_raw, breaks = seq(0, 1, .1)), 
         lapse = if_else(label == "Lapse", 1, 0)) |> 
  count(bins)
```

# Introduction


# Methods

## Transparency and Openness
<!-- reference registered report-->
<!-- add transparency report to supplement if applicable-->
We adhere to research transparency principles that are crucial for robust and replicable science. We reported how we determined the sample size, all data exclusions, all manipulations, and all study measures. Our data, questionnaires, and other study materials are publicly available on our OSF page ([https://osf.io/zvm7s/overview](https://osf.io/zvm7s/overview)), and our annotated analysis scripts and results are publicly available on our study website ([https://jjcurtin.github.io/study_risk2/](https://jjcurtin.github.io/study_risk2/)).


## Participants


## Procedure


## Measures


### Individual Characteristics


### Ecological Momentary Assessment


## Geolocation Sensing


## Data Analytic Strategy
Data preprocessing, modeling, and Bayesian analyses were done in R using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020; @kuhnTidyposteriorBayesianAnalysis2022; @goodrichRstanarmBayesianApplied2023]. Models were trained and evaluated using high-throughput computing resources provided by the University of Wisconsin Center for High Throughput Computing [@chtc].

### Model Selection
The best model configuration was selected using 6 repeats of participant-grouped 5-fold cross-validation. Folds were stratified by a between-subject measure of our outcome (no lapse vs. any lapse). Model configurations differed on: 

1. Statistical algorithm (and hyperameter tuning)

2. Feature set

3. Resampling of minority outcome


### Model Evaluation
We used a Bayesian hierarchical generalized linear model to estimate the posterior distribution and 95% credible intervals (CI) for the 30 held-out test sets for our best performing model. We used weakly informative, data-dependent priors to regularize and reduce overfitting. Random intercepts were included for repeat and fold (nested within repeat).

Using the same 30 held-out test sets, we calculated the median posterior probability and 95% Bayesian CI for auROC for each model separately by gender (not male vs. male), race/ethnicity (Hispanic and/or non-White vs. non-Hispanic White), income (below poverty line vs. above poverty line), geographic location (small town/rural vs. urban/suburban), and education (high school or less vs. some college). We conducted Bayesian group comparisons to assess the likelihood that each model performs differently by group.

### Calibration
We calibrated our probabilities using Platt scaling. We calculated Brier scores to assess the accuracy of our raw and calibrated probabilities for our best model. Brier scores range from 0 (perfect accuracy) to 1 (perfect inaccuracy).

### Feature Importance

# Results

## Participants


## Performance

The best model configuration was a down-sampled xgboost statistical algorithm that used only the dynamic feature set (i.e., features from EMA and geolocation). The median posterior probability for the best model was `r round(pp_perf$pp_median[1], 2)`, with narrow 95% CI ([`r round(pp_perf$pp_lower[1], 2)`, `r round(pp_perf$pp_upper[1], 2)`]).  

We compared our best model's performance to a baseline model that only used information gathered at baseline (demographics and individual differences in OUD characteristics) to evaluate the predictive value of adding dynamic features. The median posterior probability for the baseline model was  `r round(pp_perf$pp_median[2], 2)` (95% CI [`r round(pp_perf$pp_lower[2], 2)`, `r round(pp_perf$pp_upper[2], 2)`]). A Bayesian model comparison revealed extremely strong evidence that the best model was more predictive than the baseline model (probability = 1.00).

Our fairness subgroup comparisons revealed no evidence that performance meaningfully differed by education (probability = 0.51), weak evidence that performance differed by geographic location (probability = 0.73), and strong evidence that performance differed by gender, income, and race and ethnicity (probabilities > 0.99). Notably, our model performed better for individuals with an annual income below the federal poverty line compared to individuals above the federal poverty line, thus favoring the disadvantaged group. While differences in performance estimates exist across subgroups, they are not likely clinically meaningful as all of our subgroups yielded median auROCs between 0.92 - 0.94 (@fig-1).

{{< embed notebooks/mak_figures.qmd#fig-1 >}}


## Calibration
Our raw model predicted probabilities were generally well-calibrated (brier score = `r round(yardstick::brier_class(probs, truth = label, prob_raw)$.estimate, 3)`). We attempted to improve calibration with Platt scaling but results were comparable (brier score = `r round(yardstick::brier_class(probs, truth = label, prob_logi)$.estimate, 3)`). Calibration plots for the raw uncalibrated probabilities and calibrated probabilities with Platt scaline are presented in @fig-2. 

{{< embed notebooks/mak_figures.qmd#fig-2 >}}

## Feature Importance

{{< embed notebooks/mak_figures.qmd#fig-3 >}}

# Discussion



# References
::: {#refs}
:::
