---
title: Using smartphone sensing and machine learning for personalized daily lapse risk prediction in a national sample of people with opioid use disorder
author:
  - name: Kendra Wyant 
    corresponding: false
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison
  - name: John J. Curtin 
    corresponding: true
    email: jjcurtin@wisc.edu
    affiliations:
      - Department of Psychology, University of Wisconsin-Madison 
financial-support: This research was supported by a grant from the National Institute on Drug Abuse (NIDA; R01 DA047315 to John J. Curtin).
keywords:
  - Substance use disorders
  - Precision mental health 
abstract: |
  Enter abstract here. 
date: last-modified
number-sections: false 
editor_options: 
  chunk_output_type: console
format:
  html: default
  docx: default
  apaish-typst: 
    documentmode: man
citeproc: true
bibliography: references.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
---

```{r}
#| echo: false
#| message: false

library(tidyverse)
suppressPackageStartupMessages(source("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true"))

path_models <- format_path(str_c("risk2/models/combined"))
  
pp_perf <- read_csv(here::here(path_models, "pp_perf_tibble_xgboost_v8.csv"),
                    show_col_types = FALSE) 

probs <- read_rds(here::here(path_models, "preds_xgboost_kfold_6_x_5_v8_dyn_dem_oud.rds"))

preds_all_raw <- probs |> 
  mutate(bins = cut(prob_raw, breaks = seq(0, 1, .1)), 
         lapse = if_else(label == "Lapse", 1, 0)) |> 
  count(bins)
```

# Introduction


# Methods

## Transparency and Openness
We adhere to research transparency principles that are crucial for robust and replicable science. First, we published this study's protocol as a registered report (International Registered Report Identifier [IRRID]: DERR1-10.2196/29563) during the initial enrollment of pilot participants [@moshontzProspectivePredictionLapses2021]. Second, we followed the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis extension for Artificial Intelligence (TRIPOD+AI) guidelines [@collinsTRIPODAIStatementUpdated2024]. A TRIPOD+AI checklist is available in the supplement.<!--add checklist--> Finally, our features, labels, questionnaires, and other study materials are publicly available on our OSF page ([https://osf.io/zvm7s/overview](https://osf.io/zvm7s/overview)) and our annotated analysis scripts and results are publicly available on our study website ([https://jjcurtin.github.io/study_risk2/](https://jjcurtin.github.io/study_risk2/)).
<!--JJC: Data??-->

## Participants
<!--JJC: 451 is who we screened?   Need more detail on how to get from 451 to our sample.  Also, 451 should probably be minimized and not the starting sample or else it will look like we excluded 1/3 of the sample.  We need to distinguish between who we screened, enrolled and who engaged with the study (one month of participation).  Then we can talk about who we "excluded" from among those who were on study for 1+ months-->

We recruited 451 participants in early recovery from opioid use disorder from across the United States. We recruited through national digital advertising and collaborations with treatment providers at MOUD clinics. Our recruitment strategy was designed to create a diverse sample with respect to demographics (gender, age, race, and ethnicity), and geographic location (urban and rural). We required participants:

- were age 18 or older,
- could read, write, and speak in English,
- were enrolled in and adherent with an MOUD program for at least 1 month but no longer than 12 months or enrolled in or recently completed a day treatment program, and
- had an Android smartphone with an active data plan.  

We did not exclude participants for comorbid substance use or other psychiatric disorders. Participants were compensated up to \$75 per month for completing study tasks (i.e., daily surveys, monthly surveys and sharing sensing data) and were paid \$50 per month to offset the cost of maintaining a cellphone plan. 

<!--JJC: isnt the compensation more complicated than this with compliance bonuses, etc?-->

## Procedure
Participants completed three video or phone visits over approximately 12 months. During the enrollment visit, study staff obtained written informed consent and collected demographic information. They walked participants through how to download the Smart Technology for Addiction Recovery (STAR) study app, provided a set of video tutorials to learn how to use the app, and instructed participants to complete the intake survey within the app. The STAR app was developed by the UW Madison Center for Health Enhancement Systems Studies (CHESS) and used for all data collection. Within the app participants could control their data sharing options, monitor completed study tasks, receive reminder notifications about tasks, message staff, and access CHESS's <!--JJC: STAR?-->  suite of resources and tools for people in recovery from AUD [@gustafsonSmartphoneApplicationSupport2014]. Enrolled participants met with study staff one week later to troubleshoot technical issues. At the end of the study enrollment period participants met briefly with study staff for a debriefing session. While on study, participants were expected to complete daily surveys, monthly surveys, and share geolocation sensing data. Other sensing data streams (i.e., daily video check-ins, cellular communications, and app usage data) were collected as part of the parent grant's aims (NIDA R01 DA047315). All procedures were approved by the University of Wisconsin-Madison Institutional Review Board (Study #2019-0656). 


## Measures

### Individual Differences
We collected self-report information about demographics (age, gender, sexual orientation, race, ethnicity, education, employment, and income). Zip codes from participants' reported home addresses were linked to Rural–Urban Commuting Area (RUCA) codes to characterize the rural–urban status of their residences [@economicresearchserviceusdepartmentofagricultureRuralUrbanCommutingArea]. These variables were collected to characterize our sample. Demographic variables with some evidence for influencing OUD treatment access and clinical outcomes and/or carrying general societal stigma (gender, race/ethnicity, income, sexual orientation, and geographic location) were included as features in our model to reduce potential bias from subgroup effects being encoded indirectly by proxy variables [@pinedoCurrentReexaminationRacial2019; @kilaruIncidenceTreatmentOpioid2020; @olfsonHealthcareCoverageService2022; @greenfieldSubstanceAbuseTreatment2007; @martinNeedReceiptSubstance2022<!--add cite for orientation-->]. We also used these features to perform subgroup analyses for evaluating model fairness.
<!--JJC: so we dont include all demos in model, just fairness demos?  Lets discuss this-->

We collected information about OUD history to characterize OUD severity and recovery stability across our sample. These measures included self-reported Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition OUD symptoms [@americanpsychiatricassociationDiagnosticStatisticalManual2022], past month opioid use, past month residential treatment for OUD, past month receipt of psychiatric medication, preferred opioid, preferred route of administration, and lifetime history of overdose. As part of the aims of the parent project, we collected many other trait and state measures throughout the study. A complete list of all measures can be found in our registered report [@moshontzProspectivePredictionLapses2021].
<!--JJC: You will have all the OUD info in a table, correct?  And all demos in a table-->

### Daily Survey
Participants completed a brief (1-2 minute) daily survey each day on study through the STAR app. The daily survey became available in the app at 5:00 AM CST each morning and participants had 24 hours to complete it. Participants could enable push notifications for reminder prompts to complete the survey. Each survey had 16-items that asked participants to report the date and time of any opioid use for non-medical reasons not yet reported. <!--JJC: probably should provide more info about this.   6 hour windows-->These reports served as the primary outcome for the lapse risk prediction model. Participants also reported any other drugs that they had used and whether they took their MOUD as prescribed in the past 24 hours. Next, participants rated the maximum intensity of recent (i.e., in the past 24 hours) experiences of pain, craving, risky situations, stressful events, and pleasant events. Next, participants rated their sleep and how depressed, angry, anxious, relaxed, and happy they have felt in the past 24 hours. Lastly, participants responded to 2 future-facing items that asked about participants’ motivation and confidence to continue to avoid using opioids for non-medical reasons over the next week. The full daily survey is available in the supplement.

### Geolocation Sensing
The STAR app passively recorded participants’ time-stamped geolocations (i.e., latitude and longitude) every 1.5-15 minutes, depending on their movement. We augmented the geolocation data with self-reported subjective context. On each monthly survey we asked a set of 6 questions about frequently visited locations (i.e., locations that the participant spent more than 3 minutes on 2 or more times in a month) from the previous month. Participants were asked to describe the type of place, what they typically do there, the general frequency of pleasant and unpleasant experiences associated with the place, and the extent to which spending time there supports or undermines their recovery. <!--more detail about monthly survey maybe?  This also included measures collected at enrollment that were expected to change.  Maybe indicate this was shared too, with examplar questions about location?  Also, was there a cap on the number of locations that were queried on any survey?-->

## Data Analytic Strategy
Data preprocessing, modeling, and Bayesian analyses were done in R using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020; @kuhnTidyposteriorBayesianAnalysis2022; @goodrichRstanarmBayesianApplied2023]. Models were trained and evaluated using high-throughput computing resources provided by the University of Wisconsin Center for High Throughput Computing [@chtc].

### Feature Engineering
Features were calculated using only data collected before the start of each prediction window to ensure our models were making true future predictions. We calculated a total of 793 features from three data sources: 

1. *The prediction window day.* We created dummy-coded <!--JJC; or one-hot? I prefer the latter these days for any regularized algorithm--> features for day of the week for the start of the prediction window.

2. *Demographic individual differences.* For demographics, we created ordinal features for income based on predefined response ranges (see @tbl-1 for ranges), an ordinal feature for RUCA code associated with home address (range 1-10), and dummy coded features for gender (male vs. not male), race and ethnicity (non-Hispanic White vs. Hispanic and/or not White), and orientation (heterosexual vs. not heterosexual).  
<!--JJC:  again lets discuss other demos-->

3. *Dynamic survey and geolocation sensing data.* For both sets of dynamic <!--JJC:what are "dynamic features"?--> features we calculated two types of features: raw and difference features over varying feature scoring epochs. Raw features represent the feature value calculated within a given scoring epoch (e.g., the maximum urge rating reported on the daily survey during the 48 hours immediately preceding the start of the prediction window). Difference features capture participant-level changes from their baseline scores. Specifically, we subtracted each participant’s mean score for each feature using all available data prior to the prediction window from the associated raw feature (e.g., the participant’s average urge rating across all prior daily surveys subtracted from the maximum urge rating in the preceding 48 hours).

We used three feature scoring epochs (48, 72, and 168 hours before the start of the prediction window) to create features from the daily survey. We calculated raw and difference features for min <!--JJC:  I have stopped using min features in optimize.  Worth checking if they help here?-->, max, median, and most recent responses for the 13 5-point likert scale items (items 4-16; pain, urge, risky situation, stressful event, pleasant event, sleep, depressed, angry, anxious, relaxed, happy, abstinence motivation, and abstinence confidence). We also calculated raw and difference rate features based on counts of previous opioid lapses, other drug use, missed MOUD doses (items 1-3), and completed daily surveys in each feature scoring epoch (e.g., counts of opioid lapses within the 48 hours preceding the start of a prediction window divided by 48).

We used six feature scoring epochs (6, 12, 24, 48, 72, and 168 hours before the start of the prediction window) to create features from the densely sampled geolocation data. Raw geolocation points were cross-checked against known locations with reported subjective context. We used a threshold of 50 meters for matching context to geolocation points. We calculated raw and difference features for sum duration of time spent in transit (i.e., moving faster than 4 miles per hour) <!--JJC: this seems like an odd feature-->, time out in the evenings (i.e., not at their home between the hours of 7:00pm-4:00am). We calculated raw and difference features for sum duration of time spent at locations according to what they indicated they do at the location (spend time with friends, socialize with new people, religious activities, relax, spend time with family, volunteer, receive mental health care, receive physical health care, receive MOUD treatment, drink alcohol, take classes, work), how pleasant and unpleasant their experiences typically are at the location (Always, Most of the time, Sometimes, Rarely, Never), and how helpful and harmful the location is to their recovery (Extremely, Considerably, Moderately, Mildly, Not at all). We also calculated a raw and difference feature of location variance (i.e., the extent to which a participant's location changes over a feature scoring epoch). <!--JJC:  We should definitely share these context questions at a minimum even if we dont share the full monthly survey-->

All features (sets 1-3 above) were included in our full model. We also fit a baseline model that excluded the dynamic daily survey and geolocation features (i.e., only using feature sets 1 and 2 above) to assess the incremental predictive value of these sensing features. <!--JJC: Be clearer that this baseline model only includes demos--> 

Other feature engineering steps performed during cross-validation included imputing missing values (median imputation for numeric features and mode imputation for nominal features), dummy coding nominal features, normalizing features to have a mean of zero and standard deviation of 1, bringing outlying values (|z-score| > 5) to the fence, and removing zero and near-zero variance features as determined from held-in data. We selected coarse median/mode methods for handling missing data due to the low rates of missing values and computational costs associated with more advanced forms of imputation (e.g., KNN imputation, multiple imputation). A sample feature engineering script (i.e., tidymodels recipe) containing all feature engineering steps is available on our OSF study page.

### Lapse Labels
Prediction windows started at 6:00am in participants' local timezones and ended at 5:59am the next day. This window start time was selected to more closely approximate a typical wake-sleep cycle as opposed to midnight-to-midnight calendar day. For each participant the first prediction window began at 6:00am on their second day of participation and rolled forward day-by-day until their participation ended (i.e., the last prediction window ended at 5:59am on the day of their last recorded daily survey). <!--JJC: day before their last survey?-->

Participants reported lapses on the first daily survey item. If participants responded "yes" to the question "Have you used any opioids for non-medical reasons that you have not yet reported?", they were prompted to select the day(s) and time(s) of the lapse(s). Times were reported in 6-hour increments (12:00am–5:59am, 6:00am–11:59am, 12:00pm–5:59pm, 6:00pm–11:59pm). These responses were used to label each prediction window as lapse or no lapse. For example, if a participant reported a lapse on December 12 from 12:00am-5:59am the prediction window spanning 6:00am on December 11 through 5:59am on December 12 was labeled as a lapse.
<!--JJC: OK, I see you report more info about this daily survey item here.  You might want to consider if it belongs here or above with the other info about this item-->

### Model Configurations
<!--want to talk about initially considering all these algorithms but preliminary analyses suggested that XGBoost was superior and SHAP is optimized for XGBoost so we went with only that algorithm for the primary nested cross validation-->
We initially considered five statistical algorithms that differed in terms of assumptions and bias-variance tradeoff: elastic net, random forest, XGBoost, a single-layer neural network, and a support vector machine. We chose the optimal algorithm using 6 repeats of 5-fold cross-validation. XGBoost emerged as our best performing algorithm and was the only algorithm considered for final model selection and evaluation (see supplement for 6x5 cross-validation performance estimates of all algorithms considered).  

Final candidate model configurations differed across sensible values for key hyperparameters and outcome resampling method (i.e., up-sampling and down-sampling of the outcome at ratios ranging from 5:1 to 1:1). All resampling was exclusively done in the held-in training data (i.e., held-out data were not resampled) to prevent biasing performance estimates [@vandewieleOverlyOptimisticPrediction2021]. All model configurations were fit for both the full model using all available features and the baseline model that ablated the dynamic sensing features (see Feature Engineering section). <!--JJC: Should we know about CV before discussing issues of held-in.out?  I am already wondering about grouping now that I am reading about heldin/out-->

### Cross-Validation
We used nested cross-validation for selection and evaluation of the final candidate model configurations. We used 2 repeats of 5-fold cross-validation on the inner loop for model selection and 6 repeats of 5-fold cross-validation on the outer loop for evaluation. Participants were grouped so that all of their data were always in the held-in or held-out fold to avoid bias introduced when predicting a participant’s data from their own data. Folds were stratified so that all folds contained comparable proportions of individuals who lapsed while on study.<!--what it stratified just on lapse vs. no-lapse or something more complicated?  How does that compare to what we do with RISK-->

### Model Evaluation
We evaluate the best full model’s probability predictions across three domains: discrimination, calibration, and overall performance. We follow recommendations for reporting measures and plots to characterize these performance domains [@vancalsterEvaluationPerformanceMeasures2025]. Although, classification and clinical utility are two other important domains for evaluating a model intended to make or inform a decision (e.g., whether to send a support message to an individual). In applications of our model there would be no decision to be made. Every day an individual would receive a message regardless of their lapse risk, equivalent to a treat all condition. Therefore, we have constrained our evaluation to focus on the probability estimates for evaluating performance. <!--this needs more info if you are going to include it.  Its not clear from this brief sentence how we are not making decisions or need to care about clinical utility.  I would also probably move that part to a footnote to note have it be as salient-->

#### auROC and Model Comparisons
Our performance metric for model selection and evaluation was the area under the receiver operating characteristic curve (auROC). auROC represents the probability that the model will assign a higher predicted probability to a randomly selected positive case (lapse) compared to a randomly selected negative case (no lapse). auROC is an aggregate measure of discrimination across all possible decision thresholds. This is important because optimal thresholds depend on the setting, outcome prevalence, and the relative costs of misclassification, and should be addressed separately (e.g., with a decision curve analysis). <!--JJC: and didnt you say we wouldnt recommend classifying at all?-->

We used a Bayesian hierarchical generalized linear model to estimate the posterior distribution and 95% credible intervals (CI) for auROC for the 30 held-out test sets (i.e., held-out data in the outer loop of the nested cross-validation procedure) for our best full and baseline models. We used weakly informative, data-dependent priors to regularize and reduce overfitting. Priors were set as follows: residual standard deviation ~ normal(location=0, scale=exp(2)), intercept (after centering predictors) ~ normal(location=2.3, scale=1.3), the two coefficients for window width contrasts ~ normal (location=0, scale=2.69), and covariance ~ decov(regularization=1, concentration=1, shape=1, scale=1). We set two random intercepts to account for our resampling method: one for the repeat, and another for the fold nested within the repeat. auROCs were transformed using the logit function and regressed as a function of model contrast (full vs. baseline). From the Bayesian model we obtained the posterior distribution for auROC for the full and baseline models. We reported the median posterior probability for auROC and 95% CIs for each model. We then conducted a Bayesian model comparison to determine the probability that the full and baseline models’ performances differed systematically from each other.

We performed five dichotomous subgroup analyses to assess the fairness of our model's predictions. Using the same 30 held-out test sets and the same modeling procedure as above, we calculated the median posterior probability and 95% CI for auROC for each model separately by gender (not male vs. male), race/ethnicity (Hispanic and/or non-White vs. non-Hispanic White), income (less than \$25,000 vs. more than \$25,000)<!--We usually refer to this comparison as below poverty but our lowest income group is technically above the federal poverty line ($15,960)-->, sexual orientation (heterosexual vs. not heterosexual), and geographic location (rural vs. urban)^[We followed guidelines from the United States Health Resources and Services Administration and define urban as an area where the primary commuting flow is within a metropolitan core of 50,000 or more people (RUCA code = 1) and rural as anything not urban (RUCA codes 2-10). [@healthresourcesandservicesadministrationHowWeDefine]. We conducted Bayesian group comparisons to assess the likelihood that each model performs differently by group.

#### Calibration and Overall Performance
To further evaluate our model's predictions, we used our inner resampling procedure (2 repeats of 5-fold cross validation) on the full data set to select a single best model configuration for the full model. The final configuration selected represents the most reliable and robust configuration for deployment. We then fit this best model on our full data set using single 5-fold cross-validation. This method allowed us to obtain a single predicted probability for each observation, while still using separate data for model training and prediction.
<!--JJC: would our life be a bit easier if we always used 1x 5 fold on the inner loop?  This is a bit confusing/distracting-->

Calibration is an indicator of how well a model's predicted probabilities correspond to the true observed outcomes. For example, a well-calibrated model that assigns a 30% lapse risk prediction should observe lapses in approximately 30% of such cases. We used Platt scaling to calibrate our full model's raw probabilities [@plattProbabilisticOutputsSupport1999]. We provided a calibration plot of these raw and calibrated probabilities. To characterize overall performance we reported Brier scores for the raw and calibrated probabilities. Brier scores are the mean squared difference between the predicted probabilities and observed outcome and range from 0 (perfect accuracy) to 1 (perfect inaccuracy). We also provided histograms of risk probability distributions by true lapse outcome.

### Feature Importance
Feature importance values provide insight into the features that have the most influence on the model's predictions. For every prediction, we can extract feature importance values providing actionable insight into intervenable targets for lapse risk (i.e., for a specific individal at a specific moment). We used the same single 5-fold cross-validation procedure (see Calibration and Overall Performance section) to calculate raw Shapley values for each observation in held-out data [@lundbergUnifiedApproachInterpreting2017]. The magnitude of the raw Shapley value indicates how much the feature score for that observation adjusted the prediction (in log-odds units) relative to the mean prediction across all observations. Positive Shapley values indicate that the feature score increased the prediction for that observation and negative values indicate that the feature score decreased the prediction. In other words, higher Shapley values suggest the feature increases lapse risk and lower values suggest the feature decreases lapse risk. Shapley values are inherently additive. For any observation, Shapley values can be summed to create a total adjustment score for the predicted value. We created feature categories by collapsing features that differed only by scoring epoch and/or dummy-coded level into a single feature category. We plotted raw Shapley values and feature categories as partial dependence plots to illustrate these feature-risk relationships. 

Feature importance values can also be aggregated across all participants and all observations to provide a relative rank ordering of the most important features. We calculated overall feature importance in two ways. First, we used a traditional approach in which we calculated the mean absolute Shapley value for each feature category across all observations. This approach summarizes overall feature importance by averaging the magnitude of each feature's contribution. However, it can be skewed toward features that exhibit infrequent but very large Shapley values, potentially overstating the importance of features that are strongly associated with the outcome, but only come up in a small subset of observations. The second way we calculated feature importance was by calculating the proportion of observations in which each feature category had the highest Shapley value. This approach summarizes how frequently a feature category is influential across observations (i.e., considering both magnitude and prevalence). We provided a plot of the relative ranking of feature categories by their overall feature importance using these two methods.

# Results

## Participants
We recruited <!--JJC:screened??-->451 participants across 47 states in the United States from April 2021 through December 2024. A total of 334 participants were eligible, consented, and remained on study for at least one month <!--JJC: Do we need further justification of the one month criterion?  How many did we lose for that reason?-->. Of these, 7 participants were excluded due to evidence of careless responding on daily surveys, 10 participants were excluded due to unusually low geolocation sensing adherence (fewer than 20 geolocation points per day on average), and 14 participants were excluded due to insufficient context data for geolocation points (fewer than two contextualized locations). Our final analysis sample consisted of 303 participants. All data exclusions are documented in more detail in the supplement. Participant demographic and OUD characteristics is presented in @tbl-1.

{{< embed notebooks/mak_tables.qmd#tbl-1 >}}

## Adherence, Features, and Labels
Mean days on study across participants was 297 days (range 32-395 days). 83% of participants (249/299) remained on study for at least six months. Daily survey adherence was high. On average participants completed 73% of the daily survey prompts (range 24-100%). Participants provided, on average, 311 daily geolocation points (range 20-825). 

Our final feature set consisted of 674 features. The proportion of missing values across features was low (mean=.02, range = 0-.11<!--report as percentage as its clear I think-->). Across participants we generated a total of 88,607 day-level labels. Training sets (N=30) had, on average, 70886 labels (range 69025-73129) from 239 participants (range 239-240) <!--JJC: this is training sets from the outer loop?-->. Forty percent of participants (119/299) reported an opioid lapse while on study (mean=5.36, range 0-76). This resulted in 1.81% of the labels positive for lapse (1,603/88,607 labels). We stratified the data on a variable of whether someone lapsed on study to ensure our imbalanced outcome was evenly split over folds (mean=.018, range =.016-.020)<!--JJC: you move back and forth between percentages and proportions.  Stick with one. I think percentages are clearer because easy to report the units %-->


## auROC and Model Comparisons
The best full model configuration used an xgboost statistical algorithm and up-sampled the minority class.^[The best model configuration used 1:1 upsampling of the minority class and the following hyperparameter values: learning rate = .01, tree depth = 5, mtry = 50.] The median posterior probability for the best model was `r round(pp_perf$pp_median[1], 2)`, with narrow 95% CI ([`r round(pp_perf$pp_lower[1], 2)`, `r round(pp_perf$pp_upper[1], 2)`]). <!--JJC: This is confusing. For model comparisons, there are 30 best configurations right.  There is only ONE figuration for the final final model but that is only used for calibration and SHAP but not for model comparisons or fairness.  you talk about model comparisons next-->

We compared our best model's performance to a baseline model that only used day of week and demographic and OUD characteristic features to evaluate the incremental predictive value of adding the dynamic daily survey and geolocation sensing features. The median posterior probability for the baseline model was  `r round(pp_perf$pp_median[2], 2)` (95% CI [`r round(pp_perf$pp_lower[2], 2)`, `r round(pp_perf$pp_upper[2], 2)`]). A Bayesian model comparison revealed extremely strong evidence that the best model was more predictive than the baseline model (probability = 1.00).

Our fairness subgroup comparisons for the full model revealed no evidence that performance meaningfully differed by education (probability = 0.55) and strong evidence that performance differed by gender, income, race and ethnicity, and geographic location (probabilities > 0.90). Notably, our model performed better for individuals with an annual income below the federal poverty line compared to individuals above the federal poverty line, thus favoring the disadvantaged group. While differences in performance estimates exist across subgroups, they are not likely clinically meaningful as all of our subgroups yielded median auROCs between 0.91 - 0.94 (@fig-1). A table of fairness subgroup comparisons is available in the supplement.

{{< embed notebooks/mak_figures.qmd#fig-1 >}}


## Calibration and Overall Performance
<!--JJC: maybe here is where you talk about the final model configuration instead?-->
The full model produced generally well-calibrated raw probabilities (brier score = `r round(yardstick::brier_class(probs, truth = label, prob_raw)$.estimate, 3)`). We attempted to improve calibration with Platt scaling and results were comparable (brier score = `r round(yardstick::brier_class(probs, truth = label, prob_logi)$.estimate, 3)`). A Calibration plot revealed our raw model predictions tended to systematically over-predict risk probabilities (@fig-2).  Platt scaling appeared to produce probabilities closer to the ideal line were predicted probabilities perfectly match observed rates. Histograms of raw risk probability distributions separately by true lapse outcome are presented in @fig-2.  

{{< embed notebooks/mak_figures.qmd#fig-2 >}}

## Feature Importance

For brevity, we are only displaying the top 30 features, as defined by the 30 largest absolute mean shapley values. In the supplement, we provide full figures of all possible feature categories.<!--mention 85 total number of categories-->

{{< embed notebooks/mak_figures.qmd#fig-3 >}}

{{< embed notebooks/mak_figures.qmd#fig-4 >}}


# Discussion

- Discuss contrast between top global features and "actual" important features (i.e., what comes up day to day as being important). What does this mean for how we traditionally conceptualize feature importance?

- day of prediction window not important for oud but important for aud (add supplemental figure of lapses by day of week and hour?)

- When discussing fairness - we don’t look at age. People over 65 is an important group to look at but they were not in our sample

- Decision not include misclassification cost analyses: acknowledge that discrimination and calibration don't equate to clinical utilility. Decision analysis tools, such as net benefit curves could be used to quantify clinical benefit at relevant probability thresholds. Our model is designed to not be used to make decisions about whether or not to treat but to provide model feedback to individuals each day to help them monitor their risk for lapse. Therefore the decision has already been made to always treat. Should these models be used to inform when to deploy more cost-intensive interventions (e.g., communicating risk level to a therapist to initiate contact) these analyses will be important next steps.



# References
::: {#refs}
:::
